{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_lgb_clf(X, y, seed, cv, metric, greater_is_better, num_boost_round, early_stopping_rounds, show,**my_scorer):\n",
    "    from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score\n",
    "    from sklearn.metrics import make_scorer\n",
    "    \n",
    "    '''\n",
    "    оптимизирует гиперпараметры lightgbm с ранней остановкой и возможностью задать кастомную метрику качества\n",
    "    \n",
    "    параметры:\n",
    "        1) X -признаки\n",
    "        2) y - таргет\n",
    "        3) seed - генератор случайных чисел\n",
    "        4) cv - схема валидации\n",
    "        5) metric - оптимизируемая метрика\n",
    "        6) greater_is_better - направление улучшения значений метрики\n",
    "        7) num_boost_round - ранняя остановка\n",
    "        8) early_stopping_rounds - ранняя остановка\n",
    "        9) show - показывать процесс оптимизации\n",
    "        10) кастомная метрика (необязательный аргумент)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "        \n",
    "    ###############################################################################################################    \n",
    "    def _fit_grid(X, y, seed, cv, metric, greater_is_better, grid, num_boost_round, early_stopping_rounds, show):\n",
    "        if greater_is_better:   \n",
    "            best_metric = 0\n",
    "        else:\n",
    "            best_metric = np.inf            \n",
    "        for parameters in tqdm_notebook(ParameterGrid(grid)):\n",
    "            estimator = LGBMClassifier(random_state = seed, **params)\n",
    "            metrics = []\n",
    "            try:\n",
    "                for tr_idx, val_idx in cv.split(X):\n",
    "                    data_tr = lgb.Dataset(X[tr_idx], label=y[tr_idx])\n",
    "                    data_val = lgb.Dataset(X[val_idx], label=y[val_idx])\n",
    "                    estimator.train(parameters,\n",
    "                                    data_tr,\n",
    "                                    valid_sets=data_val,\n",
    "                                    num_boost_round=num_boost_round,\n",
    "                                    early_stopping_rounds=early_stopping_rounds)\n",
    "                    if metric =='roc_auc':\n",
    "                        metrics.append(roc_auc_score(y[val_idx], estimator.predict_proba(data_val)[:, 1]))\n",
    "                    if metric == 'f1':\n",
    "                        metrics.append(f1_score(y[val_idx], estimator.predict(data_val)))\n",
    "                    if metric == 'accuracy':\n",
    "                        metrics.append(accuracy_score(y[val_idx], estimator.predict(data_val)))\n",
    "                    if metric == 'precision':\n",
    "                        metrics.append(precision_score(y[val_idx], estimator.predict(data_val)))\n",
    "                    if metric == 'recall':\n",
    "                        metrics.append(recall_score(y[val_idx], estimator.predict(data_val)))\n",
    "                    if metric == 'custom':\n",
    "                        metrics.append(**my_scorer(y[val_idx], estimator.predict(data_val)))\n",
    "                    if metric == 'custom_prob':\n",
    "                        metrics.append(**my_scorer(y[val_idx], estimator.predict_proba(data_val)[:, 1]))\n",
    "                        \n",
    "                mean_metric = np.mean(metrics)\n",
    "                if greater_is_better:                    \n",
    "                    if mean_metric>best_metric:\n",
    "                        best_metric = mean_metric\n",
    "                        if show:\n",
    "                            print('best score = {}'.format(best_metric))\n",
    "                        best_params = parameters\n",
    "                else:\n",
    "                    if mean_metric<best_metric:\n",
    "                        best_metric = mean_metric\n",
    "                        if show:\n",
    "                            print('best score = {}'.format(best_metric))\n",
    "                        best_params = parameters\n",
    "                    \n",
    "            except:\n",
    "                for tr_idx, val_idx in cv.split(X, y):\n",
    "                    data_tr = lgb.Dataset(X[tr_idx], label=y[tr_idx])\n",
    "                    data_val = lgb.Dataset(X[val_idx], label=y[val_idx])\n",
    "                    estimator.train(parameters,\n",
    "                                    data_tr,\n",
    "                                    valid_sets=data_val,\n",
    "                                    num_boost_round=num_boost_round,\n",
    "                                    early_stopping_rounds=early_stopping_rounds)\n",
    "                    if metric =='roc_auc':\n",
    "                        metrics.append(roc_auc_score(y[val_idx], estimator.predict_proba(data_val)[:, 1]))\n",
    "                    if metric == 'f1':\n",
    "                        metrics.append(f1_score(y[val_idx], estimator.predict(data_val)))\n",
    "                    if metric == 'accuracy':\n",
    "                        metrics.append(accuracy_score(y[val_idx], estimator.predict(data_val)))\n",
    "                    if metric == 'precision':\n",
    "                        metrics.append(precision_score(y[val_idx], estimator.predict(data_val)))\n",
    "                    if metric == 'recall':\n",
    "                        metrics.append(recall_score(y[val_idx], estimator.predict(data_val)))\n",
    "                    if metric == 'custom':\n",
    "                        metrics.append(**my_scorer(y[val_idx], estimator.predict(data_val)))\n",
    "                    if metric == 'custom_prob':\n",
    "                        metrics.append(**my_scorer(y[val_idx], estimator.predict_proba(data_val)[:, 1]))\n",
    "                        \n",
    "                mean_metric = np.mean(metrics)\n",
    "                if greater_is_better:                    \n",
    "                    if mean_metric>best_metric:\n",
    "                        best_metric = mean_metric\n",
    "                        if show:\n",
    "                            print('best score = {}'.format(best_metric))\n",
    "                        best_params = parameters\n",
    "                else:\n",
    "                    if mean_metric<best_metric:\n",
    "                        best_metric = mean_metric\n",
    "                        if show:\n",
    "                            print('best score = {}'.format(best_metric))\n",
    "                        best_params = parameters\n",
    "                        \n",
    "        return (best_metric, best_params)\n",
    "    ##########################################################################################################################\n",
    "    max_depths = np.arange(3, 11).tolist()\n",
    "    num_leaves = [2**i for i in range(1, 10)]\n",
    "    min_child_samples = [20, 50, 100, 500, 1000]\n",
    "    \n",
    "    grid1 = {'max_depth' : max_depths,\\\n",
    "            'num_leaves' : num_leaves,\\\n",
    "            'min_child_samples' : min_child_samples,\\\n",
    "            'n_estimators' : [500], 'learning_rate' :[.05]}\n",
    "    \n",
    "    best_metric, best_params = _fit_grid(X = X, y = y, seed = seed, cv = cv, metric = metric,\\\n",
    "                                         greater_is_better = greater_is_better,\\\n",
    "                                         grid = grid1, num_boost_round = num_boost_round,\\\n",
    "                                         early_stopping_rounds = early_stopping_rounds, show = show) \n",
    "    \n",
    "    grid2 = {'max_depth' : [best_params['max_depth']-1, best_params['max_depth'], best_params['max_depth']+1],\\\n",
    "            'num_leaves' : [best_params['num_leaves']-8, best_params['num_leaves'], best_params['num_leaves']+8],\\\n",
    "            'min_child_samples' : [best_params['min_child_samples']-10, best_params['min_child_samples'],\\\n",
    "                                   best_params['min_child_samples']+10],\\\n",
    "            'n_estimators' : [500], 'learning_rate' :np.linspace(.01, .06, 10),\\\n",
    "            'subsample' : [.5, .7, 1], 'colsample_bytree' : [.5, .7, 1]}\n",
    "    best_metric2, best_params2 = _fit_grid(X = X, y = y, seed = seed, cv = cv, metric = metric,\\\n",
    "                                         greater_is_better = greater_is_better,\\\n",
    "                                         grid = grid2, num_boost_round = num_boost_round,\\\n",
    "                                         early_stopping_rounds = early_stopping_rounds, show = show) \n",
    "    \n",
    "    return (best_metric2, best_params2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
